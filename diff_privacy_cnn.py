# -*- coding: utf-8 -*-
"""diff_privacy_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11ClXAWXKIuvv4foZRnDsqADSwGZ41Tgc
"""

!pip install opacus

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm

from opacus import PrivacyEngine
from sklearn.metrics import accuracy_score

# Step 1: Define a Simpler CNN Model
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()

        self.conv1 = nn.Conv1d(1, 128, kernel_size=1, padding=1, stride=1)
        self.conv2 = nn.Conv1d(128, 64, kernel_size=1, padding=1, stride=1)

        self.pool = nn.MaxPool1d(2)

        self.flatten = nn.Flatten()
        self.dropout = nn.Dropout(0.1)

        self.fc1 = nn.Linear(64, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64)  # Adjust the size based on your data

        x = self.flatten(x)
        x = self.dropout(x)

        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))

        return x

# Step 2: Load and Preprocess the Dataset
df = pd.read_csv('Fuel_data.csv')

features = df[['IPG2211A2N']].values.astype(float)
labels = df['FLAG'].values.astype(int)

scaler = StandardScaler()
features = scaler.fit_transform(features)

# Convert to PyTorch tensors
features_tensor = torch.FloatTensor(features).unsqueeze(1)  # Add a channel dimension
labels_tensor = torch.LongTensor(labels)

# Step 3: Split the Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(
    features_tensor, labels_tensor, test_size=0.2, random_state=42
)

# Step 4: Create DataLoader for Training
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
test_dataset = TensorDataset(X_test, y_test)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

# Step 5: Initialize and Train the Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleCNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=3e-3)

# Differential Privacy - Stochastic Gradient Descent

def loss_fn(logits, labels):
    return F.cross_entropy(logits, labels)

def dp_train(model, epochs, train_loader, noise_multiplier, max_grad_norm):

    #DP training on train set
    privacy_engine = PrivacyEngine()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    privacy_engine.make_private(
        module = model,
        optimizer = optimizer,
        data_loader = train_loader,
        noise_multiplier = noise_multiplier,
        max_grad_norm=max_grad_norm
    )

    for epoch in tqdm(range(epochs)):
        model.train()

        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            batch_x.to(device), batch_y.to(device)

            # Forward pass
            logits = model(batch_x.float())
            loss = loss_fn(logits, batch_y)

            # Backward pass
            loss.backward()

            # Optimize with DP-SGD
            optimizer.step()

def dp_eval(model, test_loader):
    # Get the number of input channels from the first batch in val_loader
    num_input_channels = next(iter(test_loader))[0].shape[1]

    # Dynamically update the number of input channels for the first convolutional layer
    model.conv1 = nn.Conv1d(num_input_channels, 128, kernel_size=1, padding=1, stride=1)

    # Evaluation on test set
    model.eval()
    val_preds, val_labels = [], []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x.to(device), batch_y.to(device)
            logits = model(batch_x.float())
            val_preds.extend(torch.argmax(logits, dim=1).tolist())
            val_labels.extend(batch_y.tolist())

    val_accuracy = accuracy_score(val_labels, val_preds)
    print(f"\n\nValidation Accuracy: {val_accuracy * 100:.2f}%")

# Step 6 : Run the model

if __name__=="__main__":

  epochs=25

  # DP hyperparameters
  noise_multiplier=1.0
  max_grad_norm=1.0

  # Train and eval
  dp_train(model, epochs=epochs, train_loader=train_loader, noise_multiplier=noise_multiplier, max_grad_norm=max_grad_norm)
  dp_eval(model, test_loader=test_loader)