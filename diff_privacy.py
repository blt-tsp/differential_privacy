# -*- coding: utf-8 -*-
"""diff_privacy_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11ClXAWXKIuvv4foZRnDsqADSwGZ41Tgc
"""

!pip install opacus

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

from tqdm import tqdm
import matplotlib.pyplot as plt

from opacus import PrivacyEngine
from opacus.layers import DPLSTM

# Step 1: Define a CNN-RNN hybrid architecture
class TimeSeriesClassification(nn.Module):
    def __init__(self, hidden_size, num_layers):
        super(TimeSeriesClassification, self).__init__()

        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1, stride=1)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1, stride=1)
        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1, stride=1)

        self.pool = nn.MaxPool1d(kernel_size=1, padding=0, stride=1)

        # Capturing temporal dependencies
        self.rnn = DPLSTM(input_size=64, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)

        self.fc1 = nn.Linear(hidden_size, 64)
        self.fc2 = nn.Linear(64, 2)

        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))

        # Rearranging for LSTM
        x = x.permute(0, 2, 1)
        _, (h_n, _) = self.rnn(x)

        x = F.relu(self.fc1(h_n[-1, :, :]))
        x = self.dropout(x)

        x = torch.sigmoid(self.fc2(x))

        return x

# Define a simple CNN
class SimpleCNN(nn.Module):
  def __init__(self):
    super(SimpleCNN, self).__init__()

    self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1, stride=1)
    self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1, stride=1)
    self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1, stride=1)

    self.flatten = nn.Flatten()
    self.pool = nn.MaxPool1d(kernel_size=1, padding=0, stride=1)

    self.dropout = nn.Dropout(0.2)

    self.fc1 = nn.Linear(128, 64)
    self.fc2 = nn.Linear(64, 32)
    self.fc3 = nn.Linear(32, 2)

  def forward(self, x):
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))
    x = self.pool(F.relu(self.conv3(x)))

    x = self.flatten(x)
    x = self.dropout(x)

    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)

    x = torch.sigmoid(x)

    return x

# Step 2: Load and Preprocess the Dataset
df = pd.read_csv('Fuel_data.csv')

features = df[['IPG2211A2N']].values.astype(float)
labels = df['FLAG'].values.astype(int)

scaler = StandardScaler()
features = scaler.fit_transform(features)

# Convert to PyTorch tensors
features_tensor = torch.FloatTensor(features).unsqueeze(1)  # Add a channel dimension
labels_tensor = torch.LongTensor(labels)

# Step 3: Split the Data into Training, Validation, and Test Sets
dataset_size = len(features_tensor)
train_size = int(0.6 * dataset_size)
val_size = int(0.2 * dataset_size)
test_size = dataset_size - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(
    TensorDataset(features_tensor, labels_tensor),
    [train_size, val_size, test_size]
)

# Create DataLoader for Training, Validation, and Testing
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

# Differential Privacy - Stochastic Gradient Descent

def loss_fn(logits, labels):
    return F.cross_entropy(logits, labels)

def dp_train(model, epochs, train_loader, max_grad_norm, epsilon, delta):

    #DP training on train set
    privacy_engine = PrivacyEngine()

    privacy_engine.make_private_with_epsilon(
        epochs=epochs,
        module = model,
        optimizer = optimizer,
        data_loader = train_loader,
        max_grad_norm=max_grad_norm,
        target_epsilon=epsilon,
        target_delta=delta
    )

    for epoch in tqdm(range(epochs)):
        model.train()

        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            batch_x.to(device), batch_y.to(device)

            # Forward pass
            logits = model(batch_x.float())
            loss = loss_fn(logits, batch_y)

            # Backward pass
            loss.backward()

            # Optimize with DP-SGD
            optimizer.step()

def dp_eval(model, test_loader):
    # Get the number of input channels from the first batch in val_loader
    num_input_channels = next(iter(test_loader))[0].shape[1]

    # Evaluation on test set
    model.eval()
    val_preds, val_labels = [], []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x.to(device), batch_y.to(device)
            logits = model(batch_x.float())
            val_preds.extend(torch.argmax(logits, dim=1).tolist())
            val_labels.extend(batch_y.tolist())

    val_accuracy = accuracy_score(val_labels, val_preds)
    return val_accuracy

# Step 6: Run the model
if __name__ == "__main__":
    epochs = 25

    # DP hyperparameters
    max_grad_norm = 1.0
    delta_value = 1e-5

    # RNN parameters
    hidden_size=64
    num_layers=2

    num_input_channels=1
    device = "cuda" if torch.cuda.is_available() else "cpu"
    epsilon_values=[0.01, 0.1, 0.5, 0.7, 1.0, 10]           # Best is eps = 0.1 to ensure strong privacy

    model = SimpleCNN().to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    accuracies = []

    for epsilon in epsilon_values:
      # Train and eval
      dp_train(model, epochs=epochs, train_loader=train_loader, max_grad_norm=max_grad_norm, epsilon=epsilon, delta=delta_value)
      accuracy = dp_eval(model, test_loader=test_loader)
      accuracies.append(accuracy)

      print(f"(ε = {epsilon:.2f}, δ = {delta_value}), accuracy = {accuracy}")

    # Plotting the accuracy vs. epsilon
    plt.plot(epsilon_values, accuracies, marker='+', color='r')
    plt.xlabel('Epsilon (ε)')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs. Epsilon')
    plt.show()